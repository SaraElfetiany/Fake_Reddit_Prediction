{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZaTk6POkXF"
      },
      "source": [
        "# Problem Formulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG7iwHNbO_HM"
      },
      "source": [
        "### What is the input?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI3RWLu5POXz"
      },
      "source": [
        "the input data representing Reddit Fake Post Detection (by Looking Only at the Title)\n",
        "\n",
        "*   conisting of only one feature that represent the title of the posts \n",
        "*   this data doesn't contain any missing data but contain some noise in the text which need to be cleaned and preprocessed\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh4P8Pf2RtC7"
      },
      "source": [
        "### What is the output?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfogqJlZSBOO"
      },
      "source": [
        "The output is the label that specify if the post in Reddit is fack or not\n",
        "\n",
        "\n",
        "*   label contain the prediction of the probability (0-1, float)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V88cNuLOSp7e"
      },
      "source": [
        "### What data mining function is required?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7P-930fTHrI"
      },
      "source": [
        "As I understand from this part of the slide\n",
        "\n",
        "\n",
        "```\n",
        "Data Mining Functions\n",
        "1. Generalization and Summarization\n",
        "2. Association and Correlation\n",
        "3. Classification & Prediction\n",
        "4. Clustering\n",
        "5. Outlier/Anomaly Analysis\n",
        "6. Time and Ordering \n",
        "7. Structure and Network Analysis\n",
        "```\n",
        "\n",
        "The data mining in this problem requires Classification & Prediction After cleaning the data by:\n",
        "* handeling any missing data\n",
        "* removing duplications\n",
        "* removing useless data (sach as text that contain very low number of characters)\n",
        "* using the regular expression library (re)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cNfm22JTIs9"
      },
      "source": [
        "### What could be the challenges?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgXlNDGyTPEt"
      },
      "source": [
        "The Challenges represented in:\n",
        "\n",
        "\n",
        "*   Missing data\n",
        "*   Noise data (sach as text that contain very low number of characters)\n",
        "*   Dealing with text feature by preprocessing and preparing it before building the models\n",
        "*   label that contain values out of 0 and 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUrv-fxXTPWj"
      },
      "source": [
        "### What is the impact?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPmpbe4wTYUz"
      },
      "source": [
        "The impact of using the raw data as it is, without cleaning and reprocessing, will result a model with low accuracy that doesn't learn well or a desired from the data in the traing stage\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The real-life impact of building a model that solve this problem is represent in limitting the number of fack posts that could be created and posted on Reddit site by detecting it and distinguish it from the real ones. only by looking at the Title.\n",
        "<br/>\n",
        "\n",
        "As the False information on the Internet has caused many social problems due to the raise of social network and its role in different domains such as politics.\n",
        "<br/>\n",
        "Solving this problem will lead to avoid the spreeding of these fack news \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yjhAPTaTYgP"
      },
      "source": [
        "### What is an ideal solution?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smtxsp6rTe6q"
      },
      "source": [
        "the ideal solution is to clean and preprocess the data before working with it\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "*   Using word-level vectorizer instead of character-level vectorizer as it gives higher performance score while training \n",
        "*   Try different model with different hyperparamter search methods and different experimental protocols to obtain an ideal solution \n",
        "*   The best model that gives higher performance score was *LogisticRegression* followed by *XGBClassifier_Pipeline_RandomizedSearchCV_with_validation_set* \n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzijuPO0fkKC"
      },
      "source": [
        "### What is the experimental protocol used and how was it carried out? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t0TqNx_fqp3"
      },
      "source": [
        "After loading the data and cleaning and preprocessing it, different experimental protocol are used through different trials:\n",
        "* using cross_val_score to measure the performance while using any classifier with character-level vectorizer and word-level vectorizer\n",
        "* using hyperparamter search method (RandomizedSearchCV) with validation set\n",
        "* setting value for k-fold cross validation cv while using GridSearchCV / RandomizedSearchCV / or BayesSearchCV\n",
        "<br/>\n",
        "\n",
        "and measure the perormance using (roc_auc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qo-uOPBfrje"
      },
      "source": [
        "### What preprocessing steps are used?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZW8oroKfysp"
      },
      "source": [
        "*   view the data and understand it\n",
        "*   using df.info() to get more insight about the data\n",
        "*   check the missing data using df.isna().sum()\n",
        "*   removing rows that contain lable = 2  \n",
        "*   handeling the text feature by applying clean_text function in order to preprocesse and prepare it before building the models through:\n",
        "  -   remove any html tags (< /br> often found)\n",
        "  -   Keep only ASCII + European Chars and whitespace, no digits\n",
        "  -   remove single letter chars\n",
        "  -   convert all whitespaces (tabs etc.) to single wspace\n",
        "  -   if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "  -   remove text that contain very low number of characters (consider it useless data)  \n",
        "  - apply feature creation with TF-IDF (word vectorizer & character vectorizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hzdweU6Bh7i"
      },
      "source": [
        "# Get Started (Importing packages & Loading the data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XrzCH4z6_ss"
      },
      "source": [
        "## Import packages "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU-40aFA048r",
        "outputId": "3bf49830-3ddf-4a00-df15-474b650fd62c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▎                            | 10 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 100 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# this line is for BayesSearchCV and using skopt package\n",
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkKhg1LT5mqX"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, precision_score,recall_score, f1_score,precision_recall_curve\n",
        "sns.set()\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.svm import SVC\n",
        "from time import time\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "\n",
        "import re\n",
        "import pickle\n",
        "import holoviews as hv\n",
        "import nltk \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoY2ymc4PyaA"
      },
      "outputs": [],
      "source": [
        "# some seeting for pandas and hvplot\n",
        "\n",
        "pd.options.display.max_columns = 100\n",
        "pd.options.display.max_rows = 300\n",
        "pd.options.display.max_colwidth = 100\n",
        "np.set_printoptions(threshold=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZOszu8Y7p1k"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxAKKxHY7voE"
      },
      "outputs": [],
      "source": [
        "# Loading the data from csv files\n",
        "\n",
        "train = pd.read_csv('data/xy_train.csv')\n",
        "test = pd.read_csv('data/x_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bJ6A7dMT7wh0",
        "outputId": "cba60fc3-ddf1-4328-ba06-7daa36a054be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-52661d77-3516-4add-893d-ed9a473a2388\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>265723</td>\n",
              "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>284269</td>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>207715</td>\n",
              "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>551106</td>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8584</td>\n",
              "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-52661d77-3516-4add-893d-ed9a473a2388')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-52661d77-3516-4add-893d-ed9a473a2388 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-52661d77-3516-4add-893d-ed9a473a2388');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       id  \\\n",
              "0  265723   \n",
              "1  284269   \n",
              "2  207715   \n",
              "3  551106   \n",
              "4    8584   \n",
              "\n",
              "                                                                                                  text  \\\n",
              "0  A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
              "1  British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
              "2  In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
              "3  Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
              "4  Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      0  \n",
              "4      0  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Look at first records of the data \n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaeVj_sIUo1K"
      },
      "outputs": [],
      "source": [
        "# drop the id column as it isn't a feature\n",
        "train = train.drop(columns=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ffpHg2wtz8Z"
      },
      "outputs": [],
      "source": [
        "# save the id of test data to use it in save the result\n",
        "test_id = test['id']\n",
        "\n",
        "# drop the id column as it isn't a feature\n",
        "test = test.drop(columns=['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z81S8-YdLCr",
        "outputId": "13db638e-7050-4b33-b511-1e1b548d4483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 60000 entries, 0 to 59999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    60000 non-null  object\n",
            " 1   label   60000 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 937.6+ KB\n"
          ]
        }
      ],
      "source": [
        "# show the information of the train dataset\n",
        "train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGjtDor2PfPb"
      },
      "source": [
        "# Data Cleaning and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zekrU2SCXSUz",
        "outputId": "a0ed9180-e307-4130-80f8-1f80cdde7da2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check if there any missing values and counting them for each feature\n",
        "train.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "lttWEOXvPgHL",
        "outputId": "5057707b-9b3c-41f7-b6ab-40a7f5aa5a26"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1aa129dc-209f-4590-8063-34e2695d75a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59995</th>\n",
              "      <td>Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59996</th>\n",
              "      <td>Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59997</th>\n",
              "      <td>Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59998</th>\n",
              "      <td>Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59999</th>\n",
              "      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>59768 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1aa129dc-209f-4590-8063-34e2695d75a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1aa129dc-209f-4590-8063-34e2695d75a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1aa129dc-209f-4590-8063-34e2695d75a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                      text  \\\n",
              "0      A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
              "1      British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
              "2      In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
              "3      Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
              "4      Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
              "...                                                                                                    ...   \n",
              "59995                Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)   \n",
              "59996                Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back   \n",
              "59997                Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no   \n",
              "59998                Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)   \n",
              "59999                Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep   \n",
              "\n",
              "       label  \n",
              "0          0  \n",
              "1          0  \n",
              "2          0  \n",
              "3          0  \n",
              "4          0  \n",
              "...      ...  \n",
              "59995      0  \n",
              "59996      1  \n",
              "59997      0  \n",
              "59998      0  \n",
              "59999      1  \n",
              "\n",
              "[59768 rows x 2 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Drop rows that contain lable = 2 \n",
        "train = train[(train[\"label\"] != 2)]\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7IaBaKdUTZj",
        "outputId": "3519ccec-7469-46a2-b02d-93b38ba5a815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# using different text preprocessing techniques\n",
        "# the reference of this cell is 873_nlp.ipynb notebook\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def clean_text(text, for_embedding=False):\n",
        "    \"\"\" steps:\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemm\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
        "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
        "    RE_ASCII = re.compile(r\"[^A-Za-z]\", re.IGNORECASE)\n",
        "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-z]\\b\", re.IGNORECASE)\n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-z,.!? ]\", re.IGNORECASE)\n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-z,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text)\n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
        "    text = re.sub(RE_WSPACE, \" \", text)\n",
        "\n",
        "    word_tokens = word_tokenize(text)\n",
        "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        words_filtered = [\n",
        "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
        "        ]\n",
        "\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0e9wdSaWV0X",
        "outputId": "117ef4e3-7ac9-464a-e1ab-5214949ed002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 37 s, sys: 119 ms, total: 37.2 s\n",
            "Wall time: 47.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Clean text\n",
        "train[\"text_clean\"] = train.loc[train[\"text\"].str.len() > 20, \"text\"]\n",
        "train[\"text_clean\"] = train[\"text_clean\"].map(\n",
        "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCnVfNnWtnAq",
        "outputId": "3e806b59-bdc4-42c6-bba1-8290d3e60842"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "text    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzZlmi23tgUd",
        "outputId": "72a4ed1c-38c1-498e-8cca-8700164feaef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 13.4 s, sys: 91.3 ms, total: 13.5 s\n",
            "Wall time: 13.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Clean text\n",
        "test[\"text_clean\"] = test[\"text\"].map(\n",
        "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "97Y-nBN9WwhC",
        "outputId": "105d9428-7022-4372-df88-152a8a0fd24c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9a1c472b-5b74-4991-b8eb-ac651907bd8a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
              "      <td>0</td>\n",
              "      <td>group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
              "      <td>0</td>\n",
              "      <td>british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
              "      <td>0</td>\n",
              "      <td>goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
              "      <td>0</td>\n",
              "      <td>obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a1c472b-5b74-4991-b8eb-ac651907bd8a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a1c472b-5b74-4991-b8eb-ac651907bd8a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a1c472b-5b74-4991-b8eb-ac651907bd8a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                                  text  \\\n",
              "0  A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
              "1  British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
              "2  In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
              "3  Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
              "4  Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
              "\n",
              "   label  \\\n",
              "0      0   \n",
              "1      0   \n",
              "2      0   \n",
              "3      0   \n",
              "4      0   \n",
              "\n",
              "                                                                                            text_clean  \n",
              "0  group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...  \n",
              "1  british prime minist theresa may nerv attack former russian spi govern conclud high like russia ...  \n",
              "2  goodyear releas kit allow ps brought heel https youtub com watch alxulk cg zwillc fish midatlant...  \n",
              "3  happi birthday bob barker price right host like rememb man said ave pet spay neuter fuckincorpor...  \n",
              "4  obama nation innoc cop unarm young black men die magic johnson jimbobshawobodob olymp athlet sho...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F68jJcqYYcI6"
      },
      "source": [
        "# Descriptive analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUnrwh4xX2OJ",
        "outputId": "a04070bc-cb01-433d-fbd3-f2c48851a6c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "one         3285\n",
              "like        3128\n",
              "new         2998\n",
              "look        2847\n",
              "color       2737\n",
              "man         2729\n",
              "get         2602\n",
              "trump       2578\n",
              "say         2347\n",
              "peopl       2316\n",
              "use         2307\n",
              "first       2248\n",
              "make        2227\n",
              "old         2226\n",
              "time        2027\n",
              "poster      2000\n",
              "found       1999\n",
              "day         1935\n",
              "war         1858\n",
              "post        1648\n",
              "world       1570\n",
              "work        1531\n",
              "show        1513\n",
              "us          1506\n",
              "american    1504\n",
              "take        1491\n",
              "life        1482\n",
              "psbattl     1470\n",
              "help        1442\n",
              "go          1420\n",
              "state       1409\n",
              "back        1369\n",
              "two         1364\n",
              "school      1345\n",
              "see         1329\n",
              "photo       1324\n",
              "made        1314\n",
              "right       1311\n",
              "save        1308\n",
              "dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from bokeh.models import NumeralTickFormatter\n",
        "# Word Frequency of most common words\n",
        "word_freq = pd.Series(\" \".join(train[\"text_clean\"]).split()).value_counts()\n",
        "word_freq[1:40]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQv8iPR2YqrS",
        "outputId": "2d8256af-1f0a-425f-c781-1ad306fa2204"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b77d0420-2dca-4761-835b-4673e36ee96e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>angriff</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>delusion</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wane</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>undament</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>miku</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hatsun</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>nfler</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>hicock</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>mccall</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wahr</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b77d0420-2dca-4761-835b-4673e36ee96e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b77d0420-2dca-4761-835b-4673e36ee96e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b77d0420-2dca-4761-835b-4673e36ee96e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      index  freq\n",
              "0   angriff     1\n",
              "1  delusion     1\n",
              "2      wane     1\n",
              "3  undament     1\n",
              "4      miku     1\n",
              "5    hatsun     1\n",
              "6     nfler     1\n",
              "7    hicock     1\n",
              "8    mccall     1\n",
              "9      wahr     1"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# list most uncommon words\n",
        "word_freq[-10:].reset_index(name=\"freq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E19IRv6sY65q",
        "outputId": "449ba100-99b2-47f7-c09d-a2f3c5d77fe2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    0.538281\n",
              "1    0.461719\n",
              "Name: label, dtype: float64"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Distribution of label\n",
        "train[\"label\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPoS0Wkth8fE"
      },
      "source": [
        "# Feature creation with TF-IDF (word vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3vyJCfxZOZa",
        "outputId": "26a40625-6e9f-4fb2-d62b-a64b9de9bd02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TfidfVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 2))"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Compute unique word vector with frequencies\n",
        "exclude very uncommon (<10 obsv.) and common (>=30%) words\n",
        "use pairs of two words (ngram)\n",
        "\"\"\"\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    analyzer=\"word\", max_df=0.3, min_df=10, ngram_range=(1, 2), norm=\"l2\"\n",
        ")\n",
        "word_vectorizer.fit(train[\"text_clean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc-LzI_emW4G",
        "outputId": "58d9400c-86c6-408c-a239-3b412bfee5c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique word (ngram) vector extract:\n",
            "\n",
            " eli       2666\n",
            "go far    3595\n",
            "bamboo     650\n",
            "wisdom    9837\n",
            "pocket    6705\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "word_vector = pd.Series(word_vectorizer.vocabulary_).sample(5, random_state=1)\n",
        "print(f\"Unique word (ngram) vector extract:\\n\\n {word_vector}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UdOSsOko2r0"
      },
      "source": [
        "# Feature creation with TF-IDF (character vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izw6KLjdo2r5",
        "outputId": "9ba4fc17-2e65-400c-92aa-6efb360297df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='char', max_df=0.3, min_df=10, ngram_range=(1, 2))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Compute unique word vector with frequencies\n",
        "exclude very uncommon (<10 obsv.) and common (>=30%) words\n",
        "use pairs of two words (ngram)\n",
        "\"\"\"\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    analyzer=\"char\", max_df=0.3, min_df=10, ngram_range=(1, 2), norm=\"l2\"\n",
        ")\n",
        "char_vectorizer.fit(train[\"text_clean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKbS2gAKo2r6",
        "outputId": "1b8eec8d-77bc-4859-81cd-9266a71ee1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique char (ngram) vector extract:\n",
            "\n",
            " xv    611\n",
            "uv    529\n",
            "bb     37\n",
            "vn    548\n",
            "ru    452\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "char_vector = pd.Series(char_vectorizer.vocabulary_).sample(5, random_state=1)\n",
        "print(f\"Unique char (ngram) vector extract:\\n\\n {char_vector}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrH36jxCrrE3"
      },
      "source": [
        "# Spliting data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKSvHQfGrwpH"
      },
      "outputs": [],
      "source": [
        "# # Sample data - 25% of data to test set\n",
        "# X_train, X_test, Y_train, Y_test = train_test_split(train[\"text_clean\"], train[\"label\"], random_state=1, test_size=0.25, shuffle=True, stratify=train[\"label\"])\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3mGdxXisQ6x"
      },
      "outputs": [],
      "source": [
        "# Y_train.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUa61wS-xL6L",
        "outputId": "9c48c4a9-d375-4adf-92ba-a8a1febd78cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original shape (59768,) (59768,)\n"
          ]
        }
      ],
      "source": [
        "# split the train data to features and lable column\n",
        "X_train = train[\"text_clean\"]\n",
        "Y_train = train[\"label\"] \n",
        "print('original shape', X_train.shape, Y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9uCc9tKkLdn"
      },
      "source": [
        "# Saving Prediction Result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmgD4S2WkTiQ"
      },
      "outputs": [],
      "source": [
        "# define function to save the csv file of the result after each trial\n",
        "def saveResult(test, test_id, classifier, fileName):\n",
        "  submission = pd.DataFrame()\n",
        "\n",
        "  submission['id'] = test_id\n",
        "\n",
        "  submission['label'] = classifier.predict_proba(test)[:,1]\n",
        "\n",
        "  submission.to_csv(fileName, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfVTsLmXsti0"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSXCFN3oF6Ww"
      },
      "source": [
        "## MLPClassifier with word-level vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-oDQ8b77e-Q"
      },
      "source": [
        "thoughts and observations for trial 0, plan for trial 1: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **MLPClassifier** with **word-level vectorizer** function for transforming each sentence in our dataset to numeric vector to easely using it with our models then using the cross_val_score to measure the performance of the classifier.\n",
        "<br/>\n",
        "\n",
        "I excepected to get an acceptable and good performance score compare to using character-level vectorizer \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj0FiJ6Hsz-r",
        "outputId": "a6e2212b-80d6-4507-f0e9-481a225a0ba1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59768, 10061)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# transform each sentence to numeric vector with tf-idf value as elements\n",
        "X_train_vec = word_vectorizer.transform(X_train)\n",
        "test_vec = word_vectorizer.transform(test['text_clean'])\n",
        "X_train_vec.get_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0gsOzPAAvzn",
        "outputId": "5a3fe97e-bb08-4f06-9bb2-ff123f04e54c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8662701064700107\n",
            "CPU times: user 31.7 s, sys: 30.9 s, total: 1min 2s\n",
            "Wall time: 47.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "clf = MLPClassifier(\n",
        "        random_state=1,\n",
        "        solver=\"adam\",\n",
        "        hidden_layer_sizes=(12, 12, 12),\n",
        "        activation=\"relu\",\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=1,\n",
        "    )\n",
        "scores = cross_val_score(clf, X_train_vec, Y_train, scoring=\"roc_auc\", cv=5)\n",
        "print(scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyzbAYEm0MJu",
        "outputId": "62422337-88eb-4f1e-cf8b-e37cdd187abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8.1 s, sys: 6.82 s, total: 14.9 s\n",
            "Wall time: 7.81 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(early_stopping=True, hidden_layer_sizes=(12, 12, 12),\n",
              "              n_iter_no_change=1, random_state=1)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "clf.fit(X_train_vec, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZY9y9ybxa_6"
      },
      "outputs": [],
      "source": [
        "saveResult(test_vec, test_id, clf, 'MLPClassifier_with_word_level_vectorizer.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZz8pZoqw9Dv"
      },
      "source": [
        "## MLPClassifier with character-level vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W6UrJ0g4uRL"
      },
      "source": [
        "thoughts and observations for trial 1, plan for trial 2: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **MLPClassifier** with **character-level vectorizer** function for transforming each sentence in our dataset to numeric vector to easely using it with our models then using the cross_val_score to measure the performance of the classifier.\n",
        "<br/>\n",
        "\n",
        "I excepected to get lower performance score compare to using word-level vectorizer \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-QdjT8Vw9EL",
        "outputId": "03c53742-fc1d-4eac-ef34-3d7746c93127"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59768, 671)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# transform each sentence to numeric vector with tf-idf value as elements\n",
        "X_train_vec = char_vectorizer.transform(X_train)\n",
        "test_vec = char_vectorizer.transform(test['text_clean'])\n",
        "X_train_vec.get_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc32xUkEw9EN",
        "outputId": "1de95a50-4a9b-446c-a179-82440761d912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7120307231946029\n",
            "CPU times: user 15.5 s, sys: 3.73 s, total: 19.2 s\n",
            "Wall time: 15.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "clf = MLPClassifier(\n",
        "        random_state=1,\n",
        "        solver=\"adam\",\n",
        "        hidden_layer_sizes=(12, 12, 12),\n",
        "        activation=\"relu\",\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=1,\n",
        "    )\n",
        "scores = cross_val_score(clf, X_train_vec, Y_train, scoring=\"roc_auc\", cv=5)\n",
        "print(scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d72eC4XB2IIy",
        "outputId": "1d97c366-b485-466c-81db-52133a6ee84f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2.02 s, sys: 390 ms, total: 2.41 s\n",
            "Wall time: 2 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(early_stopping=True, hidden_layer_sizes=(12, 12, 12),\n",
              "              n_iter_no_change=1, random_state=1)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "clf.fit(X_train_vec, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er3yufY3xuHY"
      },
      "outputs": [],
      "source": [
        "saveResult(test_vec, test_id, clf, 'MLPClassifier_with_character_level_vectorizer.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4KqfcjNj4bJ"
      },
      "source": [
        "## RandomForestClassifier with word-level vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgWeGnxA7Htj"
      },
      "source": [
        "thoughts and observations for trial 2, plan for trial 3: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **RandomForestClassifier** with its defualt hyperparameter and with **word-level vectorizer** function for transforming each sentence in our dataset to numeric vector to easely using it with our models then using the cross_val_score to measure the performance of the classifier.\n",
        "<br/>\n",
        "\n",
        "I excepected to get an acceptable and good performance score \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myfH-EUTj4bU",
        "outputId": "d76f6f76-1a91-43fb-a822-6806fc617aa8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59768, 10061)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# transform each sentence to numeric vector with tf-idf value as elements\n",
        "X_train_vec = word_vectorizer.transform(X_train)\n",
        "test_vec = word_vectorizer.transform(test['text_clean'])\n",
        "X_train_vec.get_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDzwUPB4j-fp",
        "outputId": "2571fad0-9f94-47e8-a24a-c01e79e6b62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8366559553918183\n",
            "CPU times: user 7min 31s, sys: 1.25 s, total: 7min 32s\n",
            "Wall time: 7min 35s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "clf = RandomForestClassifier(random_state=1)\n",
        "scores = cross_val_score(clf, X_train_vec, Y_train, scoring=\"roc_auc\", cv=5)\n",
        "print(scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doD0e8Wwj4bU"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "clf.fit(X_train_vec, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK4d4A88j4bV"
      },
      "outputs": [],
      "source": [
        "saveResult(test_vec, test_id, clf, 'RandomForestClassifier_with_word_level_vectorizer.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAHOO1Jj4bt8"
      },
      "source": [
        "## LogisticRegression with word-level vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13Fn_Ayt5PbJ"
      },
      "source": [
        "thoughts and observations for trial 3, plan for trial 4: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **LogisticRegression** with its defualt hyperparameter and with **word-level vectorizer** function for transforming each sentence in our dataset to numeric vector to easely using it with our models then using the cross_val_score to measure the performance of the classifier.\n",
        "<br/>\n",
        "\n",
        "I excepected to get low performance score but it accually gives me the best score across all trials \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ymBS6uq4buD",
        "outputId": "bd5c0e66-7108-4bfb-ddea-8f8198150e6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59768, 21026)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# transform each sentence to numeric vector with tf-idf value as elements\n",
        "X_train_vec = word_vectorizer.transform(X_train)\n",
        "test_vec = word_vectorizer.transform(test['text_clean'])\n",
        "X_train_vec.get_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKcrTCEN4buE",
        "outputId": "63d7cd4f-e9fb-4fa7-9851-85d4a9fc8d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8722398657969277\n",
            "CPU times: user 7.89 s, sys: 29.7 ms, total: 7.92 s\n",
            "Wall time: 7.51 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "clf = LogisticRegression(solver=\"sag\", random_state=1)\n",
        "scores = cross_val_score(clf, X_train_vec, Y_train, scoring=\"roc_auc\", cv=10)\n",
        "print(scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtsdxib44buE",
        "outputId": "c3de707d-0e8c-491f-c67b-d24830917935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 919 ms, sys: 2.21 ms, total: 921 ms\n",
            "Wall time: 920 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=1, solver='sag')"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "clf.fit(X_train_vec, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfbaBhS64buE"
      },
      "outputs": [],
      "source": [
        "saveResult(test_vec, test_id, clf, 'LogisticRegression_with_word_level_vectorizer.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAi9rBcY7ejf"
      },
      "source": [
        "## SVC with word-level vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx06kFp37gIa"
      },
      "source": [
        "thoughts and observations for trial 4, plan for trial 5: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **SVM Classifier** with its defualt hyperparameter and with **word-level vectorizer** function for transforming each sentence in our dataset to numeric vector to easely using it with our models then using the cross_val_score to measure the performance of the classifier.\n",
        "<br/>\n",
        "\n",
        "I excepected to get higher performance score but in slow time, acually it gives me a reasonable score not the highest one and takes a lot of time more than i expected\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0zAHSue7ejn",
        "outputId": "537a1856-919e-4a40-fc47-753b97f5e7ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(59768, 10061)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# transform each sentence to numeric vector with tf-idf value as elements\n",
        "X_train_vec = word_vectorizer.transform(X_train)\n",
        "test_vec = word_vectorizer.transform(test['text_clean'])\n",
        "X_train_vec.get_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FPYDuMr-Nwa",
        "outputId": "0a2f6d22-986f-42c9-9d03-3de6bc11ad74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8635064007438583\n",
            "CPU times: user 3h 3min 17s, sys: 15.1 s, total: 3h 3min 32s\n",
            "Wall time: 3h 3min 43s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "clf = SVC(probability=True)\n",
        "scores = cross_val_score(clf, X_train_vec, Y_train, scoring=\"roc_auc\", cv=3)\n",
        "print(scores.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8D2B_9E7ejn",
        "outputId": "4d5fddbc-c41b-484f-8975-8f442e114def"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2h 20min 16s, sys: 10.2 s, total: 2h 20min 26s\n",
            "Wall time: 2h 20min 58s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "SVC(probability=True)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "clf.fit(X_train_vec, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSQ_YY037ejo"
      },
      "outputs": [],
      "source": [
        "saveResult(test_vec, test_id, clf, 'SVC_with_word_level_vectorizer.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVxQoU8Nq3K9"
      },
      "source": [
        "## XGBClassifier Pipeline (RandomizedSearchCV) with validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4HuRizt18F9"
      },
      "source": [
        "thoughts and observations for trial 5, plan for trial 6: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **XGBClassifier** through the pipeline that contain the classifier and the TfidfVectorizer which used to transform each sentence of the text data to numeric vector\n",
        "with **RandomizedSearchCV** function to get the best random hyperparameters of all the hyperparameter combinations among the specified number of iteration\n",
        "and with **Validation Set** \n",
        "<br/>\n",
        "\n",
        "the hyperparameters that used in this trial:\n",
        "* 'tfidf__ngram_range': [(1, 2), (1, 3)]\n",
        "* 'tfidf__max_df': np.arange(0.3, 0.8)\n",
        "* 'tfidf__min_df': np.arange(5, 100)\n",
        "* 'n_estimators': [100, 150, 200]  \n",
        "* 'max_depth':[200, 300, 500]  \n",
        "\n",
        "<br/>\n",
        "\n",
        "I excepected to get an acceptable and good performance score\n",
        "<br/>\n",
        "Also I excepected the hyperparameters that reach the *local optimal* (within the given range) and produce the better accuracy among the random selected combinations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XWkHhQxr42m"
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "                 (\"tfidf\", TfidfVectorizer()),\n",
        "                 ('my_classifier', XGBClassifier())\n",
        "                 ])\n",
        "\n",
        "params = {\n",
        "    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n",
        "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
        "    \"tfidf__min_df\": np.arange(5, 100),\n",
        "    'my_classifier__n_estimators': [100, 150, 200],  \n",
        "    'my_classifier__max_depth':[200, 300, 500]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P_VJBrVq5st",
        "outputId": "288bdcb2-0df7-48cb-d17a-760337b2b919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 1 folds for each of 5 candidates, totalling 5 fits\n",
            "best score 0.8482701068133212\n",
            "best score {'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 9, 'tfidf__max_df': 0.3, 'my_classifier__n_estimators': 200, 'my_classifier__max_depth': 300}\n",
            "CPU times: user 31min, sys: 5.15 s, total: 31min 5s\n",
            "Wall time: 1h 42min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "\n",
        "# Split the original training set to a train and a validation set\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train, Y_train, train_size = 0.8, stratify = Y_train, shuffle=True, random_state = 2022)\n",
        "\n",
        "# Create a list where train data indices are -1 and validation data indices are 0\n",
        "# X_tr (new training set), X_train\n",
        "split_index = [-1 if x in X_tr.index else 0 for x in X_train.index]\n",
        "\n",
        "# Use the list to create PredefinedSplit\n",
        "pds = PredefinedSplit(test_fold = split_index)\n",
        "\n",
        "pipe_clf = RandomizedSearchCV(\n",
        "    pipe, params, cv=pds, verbose=1, n_jobs=2,\n",
        "    # number of random trials\n",
        "    n_iter=5,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "# here we still use X_train; but the grid search model\n",
        "# will use our predefined split internally to determine \n",
        "# which sample belongs to the validation set\n",
        "pipe_clf.fit(X_train, Y_train)\n",
        "\n",
        "print('best score {}'.format(pipe_clf.best_score_))\n",
        "print('best score {}'.format(pipe_clf.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlGx3jYZyQdA"
      },
      "outputs": [],
      "source": [
        "saveResult(test['text_clean'], test_id, pipe_clf, 'XGBClassifier_Pipeline_RandomizedSearchCV_with_validation_set.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHH7_0IrIkw9"
      },
      "source": [
        "## Parameter tuning with XGBClassifier Pipeline (RandomizedSearchCV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFOF887b_wvH"
      },
      "source": [
        "thoughts and observations for trial 6, plan for trial 7: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **XGBClassifier** through the pipeline that contain the classifier and the TfidfVectorizer which used to transform each sentence of the text data to numeric vector\n",
        "with **RandomizedSearchCV** function to get the best random hyperparameters of all the hyperparameter combinations among the specified number of iteration\n",
        "and with **cross-validation** \n",
        "<br/>\n",
        "\n",
        "the hyperparameters that used in this trial:\n",
        "* 'tfidf__ngram_range': [(1, 2), (1, 3)]\n",
        "* 'tfidf__max_df': np.arange(0.3, 0.8)\n",
        "* 'tfidf__min_df': np.arange(5, 100)\n",
        "* 'n_estimators': [100, 150, 200]  \n",
        "* 'max_depth':[100, 200, 300]  \n",
        "\n",
        "<br/>\n",
        "\n",
        "I excepected to get lower accuracy compared to the same classifier with the predefined validation set\n",
        "<br/>\n",
        "Also I excepected the hyperparameters that reach the *local optimal* (within the given range) and produce the better accuracy among the random selected combinations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pP_Yb61JNH8",
        "outputId": "bbc06bac-3a5c-4f88-9710-255bfc768dec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
            "best score 0.8357986772864227\n",
            "best score {'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 18, 'tfidf__max_df': 0.3, 'my_classifier__n_estimators': 100, 'my_classifier__max_depth': 100}\n",
            "CPU times: user 5min 2s, sys: 3.58 s, total: 5min 5s\n",
            "Wall time: 1h 41min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# feature creation and modelling in a single function\n",
        "pipe = Pipeline([(\"tfidf\", TfidfVectorizer()), ('my_classifier', XGBClassifier())])\n",
        "\n",
        "# define parameter space to test # runtime 35min\n",
        "params = {\n",
        "    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n",
        "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
        "    \"tfidf__min_df\": np.arange(5, 100),\n",
        "    'my_classifier__n_estimators': [50, 100, 200],  \n",
        "    'my_classifier__max_depth':[100, 200, 300]\n",
        "}\n",
        "\n",
        "\n",
        "pipe_clf = RandomizedSearchCV(\n",
        "    pipe, params, cv=4, verbose=1, n_jobs=2, \n",
        "    # number of random trials\n",
        "    n_iter=5,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "pipe_clf.fit(X_train, Y_train)\n",
        "pickle.dump(pipe_clf, open(\"./clf_pipe.pck\", \"wb\"))\n",
        "\n",
        "print('best score {}'.format(pipe_clf.best_score_))\n",
        "print('best score {}'.format(pipe_clf.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDROq__dK7BL"
      },
      "outputs": [],
      "source": [
        "saveResult(test['text_clean'], test_id, pipe_clf, 'XGBClassifier_Pipeline_with_RandomizedSearchCV.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCJzrwrQ0LAz"
      },
      "source": [
        "## Parameter tuning with XGBClassifier Pipeline (BayesSearchCV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2n2AiGk2HnX"
      },
      "source": [
        "thoughts and observations for trial 7, plan for trial 8: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **XGBClassifier** through the pipeline that contain the classifier and the TfidfVectorizer which used to transform each sentence of the text data to numeric vector\n",
        "with **BayesSearchCV** function to get the best hyperparameters among the specified number of iteration based on using bayesian learning to predict what is the next hyperparamter values we should try given the current trials\n",
        "<br/>\n",
        "\n",
        "the hyperparameters that used in this trial:\n",
        "* 'tfidf__max_df': np.arange(0.3, 0.8)\n",
        "* 'tfidf__min_df': np.arange(5, 100)\n",
        "* 'n_estimators': [100, 200]  \n",
        "* 'max_depth':[200, 300]   \n",
        "\n",
        "<br/>\n",
        "\n",
        "I excepected to get better accuracy compared to the RandomizedSearchCV with the same classifier\n",
        "<br/>\n",
        "Also I excepected the hyperparameters that reach the *local optimal* (within the given range) and produce the better accuracy among the generated combinations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCyEx_ff0LA2",
        "outputId": "b1d9d104-16d2-415a-aa02-841641f67d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "best score 0.8339057171638196\n",
            "best score OrderedDict([('my_classifier__max_depth', 218), ('my_classifier__n_estimators', 173), ('tfidf__max_df', 0.3), ('tfidf__min_df', 19)])\n",
            "CPU times: user 18min 16s, sys: 7.15 s, total: 18min 23s\n",
            "Wall time: 2h 16min 11s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# feature creation and modelling in a single function\n",
        "pipe = Pipeline([(\"tfidf\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 3), norm=\"l2\")), ('my_classifier', XGBClassifier())])\n",
        "\n",
        "# define parameter space to test # runtime 35min\n",
        "params = {\n",
        "    # \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n",
        "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
        "    \"tfidf__min_df\": np.arange(5, 100),\n",
        "    'my_classifier__n_estimators': [100, 200],  \n",
        "    'my_classifier__max_depth':[200, 300]\n",
        "}\n",
        "\n",
        "\n",
        "pipe_clf = BayesSearchCV(\n",
        "    pipe, params, cv=3, verbose=1, n_jobs=2, n_iter=5,\n",
        "    scoring='roc_auc')\n",
        "\n",
        "pipe_clf.fit(X_train, Y_train)\n",
        "#pickle.dump(pipe_clf, open(\"./clf_pipe.pck\", \"wb\"))\n",
        "\n",
        "print('best score {}'.format(pipe_clf.best_score_))\n",
        "print('best score {}'.format(pipe_clf.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvoxQb_w0LA4"
      },
      "outputs": [],
      "source": [
        "saveResult(test['text_clean'], test_id, pipe_clf, 'XGBClassifier_Pipeline_with_GridSearchCV.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWwHtMsyP-91"
      },
      "source": [
        "## Parameter tuning with SVC Pipeline (RandomizedSearchCV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQbsVnht09o2"
      },
      "source": [
        "thoughts and observations for trial 8, plan for trial 9: \n",
        "\n",
        "<br/>\n",
        "\n",
        "I used **SVM Classifier** through the pipeline that contain the classifier and the TfidfVectorizer which used to transform each sentence of the text data to numeric vector\n",
        "with **RandomizedSearchCV** function to get the best random hyperparameters of all the hyperparameter combinations among the specified number of iteration \n",
        "<br/>\n",
        "\n",
        "the hyperparameters that used in this trial:\n",
        "* 'tfidf__ngram_range': [(1, 2), (1, 3)]\n",
        "* 'tfidf__max_df': np.arange(0.3, 0.8)\n",
        "* 'tfidf__min_df': np.arange(5, 100)\n",
        "* 'C': np.arange(0.2, 1, 0.15)\n",
        "\n",
        "<br/>\n",
        "\n",
        "I excepected better performance score compared to the SVC with its defualt hyperparameter and with word-level vectorizer \n",
        "<br/>\n",
        "\n",
        "I excepected the hyperparameters that reach the *local optimal* (within the given range) and produce the better accuracy among the random selected combinations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q65P6TCTyQy",
        "outputId": "c2e31cff-2b3d-4bcf-97f0-37232a6d4ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best score 0.8667282140889441\n",
            "best score {'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 15, 'tfidf__max_df': 0.3, 'my_classifier__C': 0.9499999999999997}\n",
            "Wall time: 2d 7h 26min 35s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# feature creation and modelling in a single function\n",
        "pipe = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"my_classifier\", SVC(probability=True))])\n",
        "\n",
        "# define parameter space to test # runtime 35min\n",
        "params = {\n",
        "    \"tfidf__ngram_range\": [(1, 2), (1, 3)],\n",
        "    \"tfidf__max_df\": np.arange(0.3, 0.8),\n",
        "    \"tfidf__min_df\": np.arange(5, 100),\n",
        "    \"my_classifier__C\": np.arange(0.2, 1, 0.15),\n",
        "}\n",
        "# it is quite slow so we do 4 for now\n",
        "pipe_clf = RandomizedSearchCV(\n",
        "    pipe, params, n_jobs=-1, scoring=\"roc_auc\", n_iter=3)\n",
        "pipe_clf.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "print('best score {}'.format(pipe_clf.best_score_))\n",
        "print('best score {}'.format(pipe_clf.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4MM4ajTP--E"
      },
      "outputs": [],
      "source": [
        "saveResult(test['text_clean'], test_id, pipe_clf, 'SVC_Pipeline_with_RandomizedSearchCV.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os5YUiTX2UT5"
      },
      "source": [
        "# Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_AJ5LIz2Vz4"
      },
      "source": [
        "## What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SKwq3M22WBI"
      },
      "source": [
        "Word n-gram model\n",
        "* Represent **unique word sequence** of length n as\n",
        "feature\n",
        "Character n-gram model\n",
        "* Represent **unique character sequence** of length n as\n",
        "feature\n",
        "<br/>\n",
        "\n",
        "For a given document in a language, there are more character N-grams repetitions than word n-gram repetitions,\n",
        "i.e. a character N-grams split highlights some common properties that a word n-gram split does not.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Word n-gram tends to suffer more from the OOV issue as:\n",
        "\n",
        "**Out Of Vocabulary (OOV) words** refer to the new words which are encountered at testing. These new words do not exist in the vocabulary. Hence, these methods fail in handling OOV words.\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "resources: \n",
        "* https://reader.elsevier.com/reader/sd/pii/S1877042813041918?token=3E1F8BE1DC047E8C3B89A98E9A583DE265B6477EF7E4DC237DACFA7F133D2A53A3592532CD63666A37288E8E65F64116&originRegion=eu-west-1&originCreation=20220314111053\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSVJ0pbN2WNa"
      },
      "source": [
        "## What is the difference between stop word removal and stemming? Are these techniques language-dependent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QALa0PDz2qHU"
      },
      "source": [
        "Stemming\n",
        "* Stemmers remove morphological affixes from words,\n",
        "leaving only the word stem. (nltk)\n",
        "\n",
        "ex:\n",
        "* print(stemmer.stem(\"running\"))\n",
        "  - run\n",
        "* print(stemmer.stem(\"generously\"))\n",
        "  - generous\n",
        "\n",
        "<br/>\n",
        "\n",
        "Stop words removal\n",
        "* Remove high frequency words that have little semantic weight and are thus unlikely to help the retrieval process\n",
        "\n",
        "ex:\n",
        "* i, me, my, myself, we, our, ours, ourselves, you, your, yours,\n",
        "yourself, yourselves, he, him, his, himself, she, her, hers,\n",
        "herself, it, its, itself, they, them, their, theirs, …\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "Yes, both of these techniques are language-dependent. So we need to specify the language when using them\n",
        "<br/>\n",
        "example:\n",
        "\n",
        "```\n",
        "stemmer = SnowballStemmer(\"german\")\n",
        "stop_words = set(stopwords.words(\"german\"))\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYEqPag02qKZ"
      },
      "source": [
        "## Is tokenization techniques language dependent? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQZEq3Xc20z4"
      },
      "source": [
        "No, not all tokenization techniques are language dependent\n",
        "\n",
        "\n",
        "There are various tokenization techniques available which can be applicable based on the language and purpose of modeling.\n",
        "<br/>\n",
        "\n",
        "So, Tokenization is depending upon delimiters\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "The basic initial step is converting texts using Tokenization method, which ‘breaks’ raw text in smaller pieces. Tokens can be words, singles characters or subwords (n-gram characters: a contiguous sequence of n items from a given sample of text or speech). \n",
        "<br/>\n",
        "\n",
        "**The most common way of tokenization is based on space**. Taking space as a delimiter for example, you will get from “Natural Language Processing” 3 tokens : “Natural”, “Language”, “Processing”.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Major techniques for tokenizing are:\n",
        "\n",
        "Split, Spacy, Gensim, Regular Expression, NLTK (Natural Language Toolkit) library, Dictionary Based Tokenization, Rule Based Tokenization, Penn TreeBank Tokenization, and many others...\n",
        "\n",
        "<br/>\n",
        "\n",
        "Which Tokenization method to use will depend on your text (language, presence of white spaces or characters, special signs, type of genre (medical, scientific, etc.)) and tasks you need to achieve\n",
        "\n",
        "<br/>\n",
        "\n",
        "resources:\n",
        "* https://www.linkedin.com/pulse/what-tokenization-natural-language-processing-nlp-rakesh-bhol\n",
        "* https://medium.com/unpackai/natural-language-processing-tokenization-and-numericalization-63c2df20d917 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e98ZUbZm22OE"
      },
      "source": [
        "## What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fanM0fTb22Wk"
      },
      "source": [
        "TfidfVectorizer and CountVectorizer both are methods for converting text data into vectors as model can process only numerical data.\n",
        "\n",
        "In CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently.\n",
        "\n",
        "In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents.\n",
        "\n",
        "<br/>\n",
        "\n",
        "> TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. We can then remove the words that are less important for analysis, hence making the model building less complex by reducing the input dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "No, it Wouldn't be feasible to use all possible n-grams especially if it is of a significant a size\n",
        "\n",
        "\n",
        "As it depends on the application and the dataset. In some applications, it is important to deal with some combinations of words together as they are together, have a big effect. \n",
        "\n",
        "For problems like Sentiment Analysis, setting n-gram ranges that use bigrams or trigrams can dramatically improve the accuracy of classification, as they can capture more complex expressions formed by the composition of more than one word.\n",
        "\n",
        "Another example: if we have a dataset that contain some products and we interested in the product that is easy to use, then the sentence 'easy to use' is important for us, therefore we need to use the trigrams (n-gram size = 3) to capture this combination of words, so this will lead to improve the accuracy of the model.\n",
        "\n",
        "Also if we don't have a specific combination of word that is important to us, we can then experiment different value of n-grams whether doing it manually (trial and error) or by using one of hyperparamter search methods (grid/random/bayesian)\n",
        "\n",
        "<br/>\n",
        "\n",
        "resources:\n",
        "* https://www.quora.com/What-is-the-difference-between-TfidfVectorizer-and-CountVectorizer-1\n",
        "* https://help.monkeylearn.com/en/articles/2174105-n-gram-range"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "SCZaTk6POkXF",
        "EG7iwHNbO_HM",
        "Lh4P8Pf2RtC7",
        "V88cNuLOSp7e",
        "3cNfm22JTIs9",
        "wUrv-fxXTPWj",
        "4yjhAPTaTYgP",
        "_Qo-uOPBfrje",
        "2hzdweU6Bh7i",
        "-XrzCH4z6_ss",
        "CZOszu8Y7p1k",
        "fGjtDor2PfPb",
        "F68jJcqYYcI6",
        "MPoS0Wkth8fE",
        "_UdOSsOko2r0",
        "XrH36jxCrrE3",
        "Q9uCc9tKkLdn",
        "bfVTsLmXsti0",
        "KSXCFN3oF6Ww",
        "DZz8pZoqw9Dv",
        "z4KqfcjNj4bJ",
        "iAHOO1Jj4bt8",
        "zAi9rBcY7ejf",
        "WVxQoU8Nq3K9",
        "RHH7_0IrIkw9",
        "GCJzrwrQ0LAz",
        "cWwHtMsyP-91",
        "D_AJ5LIz2Vz4",
        "BSVJ0pbN2WNa",
        "NYEqPag02qKZ",
        "e98ZUbZm22OE"
      ],
      "name": "Assignment 3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
